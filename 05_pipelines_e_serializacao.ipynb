{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistindo modelos de machine learning\n",
    "\n",
    "<img src='https://www.oreilly.com/library/view/head-first-python/9781449397524/httpatomoreillycomsourceoreillyimages1368712.png.jpg'/>\n",
    "\n",
    "## Introdução\n",
    "\n",
    "Uma vez que finalizamos as análises nos dados e a etapa de experimentação com técnicas de aprendizagem de máquinas, podemos persistir nossa melhor solução para viabilizar seu uso. Nesta aula, utilizaremos mecanismos do próprio Python para serializar o modelo de identificação de sobreviventes do Titanic, que fizemos na última aula.\n",
    "\n",
    "### O que vamos fazer?\n",
    "\n",
    "- Melhorar nosso pré-processamento de dados;\n",
    "- Estabelecer pipelines;\n",
    "- Salvar nosso modelo criado e embarcá-lo numa aplicação WEB Flask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iremos continuar usando a base pública [Titanic](https://www.kaggle.com/c/titanic/data), que já está no nosso repositório (`titanic.csv`) e aplicar as principais limpezas que utilizamos na última aula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic = pd.read_csv(\"bases/titanic.csv\")\n",
    "df_titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Vamos excluir algumas colunas e também as linhas que não possuem valor de alvo (Survived)\n",
    "\n",
    "df_titanic.drop(['PassengerId', 'Name', 'Ticket', 'Fare', 'Cabin'], axis=1, inplace=True)\n",
    "df_titanic.dropna(subset=['Survived'], axis=0, inplace=True)\n",
    "df_titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines\n",
    "\n",
    "<img src='http://frankchen.xyz/images/15231783974167.jpg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso *pipeline* deverá tratar dados ausentes, e numéricos e categóricos. Dessa forma, dependendo do tipo da coluna, um tipo de pré-processamento será feito.\n",
    "\n",
    "Para isso, inicialmente, vamos identificar e isolar os atributos do nosso DataFrame por tipo (categóricos e numéricos), porém, antes iremos deixar bossos atributos de treino ($X$) e o alvo ($y$) de forma explícita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_titanic.drop([\"Survived\"], axis=1)\n",
    "y = df_titanic[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_attrs = X.select_dtypes(['object']).columns\n",
    "cat_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attrs = X.select_dtypes(['int', 'float']).columns\n",
    "num_attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muito bem, agora, iremos criar ''subpipelines'' com o seguinte fluxo:\n",
    "\n",
    "- Dados categóricos:\n",
    " - Tratar valores ausentes\n",
    "   - Iremos atribuir o valor mais comum presente no conjunto de treino\n",
    " - Codificar valores usando o `OrdinalEncoder`\n",
    "   - Dessa forma, teremos valores inteiros e compatíveis com o Scikit-Learn\n",
    " \n",
    "\n",
    "- Dados numéricos\n",
    " - Tratar valores ausentes\n",
    "   - Podemos tratar utilizando a média do conjunto de treino\n",
    " - Padronizar valores usando o `StandardScaler`\n",
    "   - Dados numéricos podem estar em escalas de valores diferentes. Uma das práticas da estatística é a normalização dos dados com a centralização da média em zero e escala em desvio padrão ($z = \\frac{(X_i - \\mu)}{\\sigma}$). Esse processo gera algo muito próximo a uma distribuição normal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "\n",
    "\n",
    "cat_transformers = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ord_encoder', OrdinalEncoder())\n",
    "])\n",
    "\n",
    "num_transformers = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('standardizer', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, iremos utilizar o `ColumnTransformer` para aplicar as transformações nas colunas corretas dos nossos conjuntos de dados. \n",
    "\n",
    "Lembre-se que já identificamos quais são as colunascategóricas e numéricas, através do método `select_dtypes()`. Elas foram armazenadas nas variáveis `cat_attrs` e `num_attrs`, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('cat', cat_transformers, cat_attrs),\n",
    "    ('num', num_transformers, num_attrs),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## APENAS PARA VISUALIZAÇÃO!!\n",
    "\n",
    "X_ = pd.DataFrame(preprocessor.fit_transform(X), \n",
    "                  columns=cat_attrs.to_list() + num_attrs.to_list())\n",
    "\n",
    "X_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show! Agora que já temos nossa etapa de pré-processamento pronta, vamos montar nosso pipeline 'principal' que irá recepcionar os dados, executar `preprocessor` e, então aplicar nosso modelo.\n",
    "\n",
    "Continuarei usando redes neurais, com os mesmos parâmetros vistos na última aula, porém sinta-se livre para usar qualquer modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "model = MLPClassifier(hidden_layer_sizes=(15, 90), activation='relu', solver='adam', \n",
    "                      max_iter=300, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline(steps=[\n",
    "    ('Pré-processamento', preprocessor),\n",
    "    ('Classificação', model)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, não podemos esquecer que precisaremos ter conjuntos de treino e teste (com seus respectivos alvos separados): \n",
    "\n",
    "- Treino: $X\\_train$, $y\\_train$\n",
    "- Teste: $X\\_test$, $y\\_test$\n",
    "\n",
    "Para nos ajudar a criar esses subconjuntos, usaremos o método [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) novamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TUDO PRONTINHO para usarmos nosso Pipeline `clf`... vamos testar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos verificar sua performance com a `Scikit-plot`?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Caso necessário, instale através do comando:\n",
    "\n",
    "! pip install scikit-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=True, ax=ax[0])\n",
    "skplt.metrics.plot_roc(y_test, y_proba, ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "\n",
    "\n",
    "print(classification_report(y_pred, y_test))\n",
    "print(\"Acurácia: %.4f\" % accuracy_score(y_pred, y_test))\n",
    "print(\"AUC: %.4f\" % roc_auc_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando meu modelo\n",
    "\n",
    "<img src='https://www.smartfile.com/blog/wp-content/uploads/2015/11/python-pickle-800x200.png'/>\n",
    "\n",
    "O módulo `pickle`, do Python, é usado para **serializar** e **desserializar** uma estrutura de objetos Python. <mark>Qualquer objeto no Python pode ser modificado para que possa ser salvo no disco.</mark> \n",
    "\n",
    "O que o pickle faz é que ele “serializa” o objeto antes de gravá-lo no arquivo. Esse processo consiste de formas de de converter um objeto python (lista, ditado etc.) em um fluxo de caracteres que contenham todas as informações necessárias para reconstruir o objeto em outro script python.\n",
    "\n",
    "- No caso específico de objetos do scikit-learn, podemos utilizar o módulo `joblib`, que é mais eficiente ao lidar com grandes matrizes numpy internamente ([mais detalhes >>](https://scikit-learn.org/stable/modules/model_persistence.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "\n",
    "joblib.dump(clf, './meu_modelo.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_disk = joblib.load('meu_modelo.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_disk.predict(X_test)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instancia = pd.DataFrame([{\n",
    "    'Pclass': 2,\n",
    "    'Sex': 'male',\n",
    "    'Age': 21,\n",
    "    'SibSp': 0,\n",
    "    'Parch': 0,\n",
    "    'Embarked': 'S'\n",
    "}])\n",
    "\n",
    "instancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_pred = clf_disk.predict(instancia)\n",
    "inst_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_proba = clf_disk.predict_proba(instancia)\n",
    "inst_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(inst_proba, explode=(0.15, 0),  autopct=\"%.2f%%\",\n",
    "        labels=['Prob. Morrer', 'Prob. Sobreviver'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos embarcar isso numa webapp?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
